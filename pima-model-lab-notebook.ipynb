{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-hour glucose tolerance test is a serious hassle and seriously unpleasant. Most people would not voluntarily undergo it were it not a standard and important part of prenatal care. If you were worried that somebody was prediabetic, there are a number of other tests you would perform first (glycated hemoglobin aka A1C, fasting plasma glucose, random plasma glucose). So, using other tests, can we predict when somebody should be getting one of those screening tests done?\n",
    "\n",
    "For the two-hour glucose tolerance test, which are the values in the Pima Indians Diabetes database, <140 mg/dL is normal, 141-200 mg/dL is prediabetes, and >200 mg/dL is diabetes. The same cutoffs apply for random plasma glucose, which are the values in the Framingham Heart Study database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import matplotlib as plt\n",
    "import seaborn as sbn\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, balanced_accuracy_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import uniform\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling data from postgres\n",
    "One minor piece of data engineering done in postgres in 'setup.sql': calling TRUE for diabetes on individuals in pima db who either had outcome = 1 (diabetes) or blood glucose > 139 mg/dL (prediabetes by clinical standards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_args = {'host': 'localhost', \n",
    "                   'dbname': 'med_data', \n",
    "                   'port': 5432}\n",
    "connection = pg.connect(**connection_args)\n",
    "\n",
    "query = \"\"\"SELECT * FROM pima\"\"\"\n",
    "\n",
    "with open('tempfile.csv', 'wb') as tmpfile:\n",
    "    copy_sql = \"COPY ({query}) TO STDOUT WITH CSV {head}\".format(\n",
    "       query=query, head=\"HEADER\"\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    cursor.copy_expert(copy_sql, tmpfile)\n",
    "    tmpfile.seek(0)\n",
    "\n",
    "    pima_data = pd.read_csv('tempfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing BMI values from skin thickness\n",
    "I know from my exploration that there are a lot of missing values coded as 0 in this dataset. I also know that skin thickness and BMI appear to be linearly related. This leads to an obvious question - can I use a linear regression to impute missing values of one from the other?\n",
    "\n",
    "I'm going to base the model on BMI rather than skin thickness; one is extremely common and straightforwardish to get, while body fat measures involving skin thickness on various parts of the body are notoriously imprecise and prone to measuring error. It's only two data points that have skin thickness but lack BMI, but it's practice setting up this sort of thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_data = pd.read_csv('diabetes.csv')\n",
    "pima_data_LR = pima_data\n",
    "pima_data_LR[['BMI', 'SkinThickness']] = pima_data_LR[['BMI', 'SkinThickness']].replace(0, np.nan)\n",
    "pima_data_LR = pima_data_LR.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(np.array(pima_data_LR['SkinThickness']).reshape(-1, 1), pima_data_LR['BMI'])\n",
    "lin_reg.predict(np.array([23, 23]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, this is a litte worrying; the average skin thickness in the sample is 29 and the average BMI is 32. Let's graph this and see how I feel about this then. (Just using seaborn's regplot for a quick look, as I would expect its results to be very similar to the results for sklearn's LinearRegressor and the graphing process is a lot more user-friendly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.regplot('SkinThickness', 'BMI', pima_data_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's actually quite reasonable. Lots of noise, but that's to be expected with a skin thickness measurement.\n",
    "\n",
    "The below is totally a case of using a bazooka to kill a mosquito, but I want to have this kind of thing in my back pocket - using a cross-validated linear regression to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place to store predictions, using code from https://towardsdatascience.com/predicting-missing-values-with-python-41e581511117\n",
    "y_pred = []\n",
    "y_true = []\n",
    "# Masking out cells with no skin thickness data or no BMI\n",
    "pima_data_BMI_impute = pima_data[(pima_data['SkinThickness'] > 0) & (pima_data['BMI'] > 0)]\n",
    "\n",
    "# Getting k-folds\n",
    "kf = KFold(n_splits=5, random_state = 36)\n",
    "for train_index, test_index in kf.split(pima_data_BMI_impute):\n",
    "    df_test = pima_data_BMI_impute.iloc[test_index]\n",
    "    df_train = pima_data_BMI_impute.iloc[train_index]\n",
    "# Setting X and y\n",
    "for train_index, test_index in kf.split(pima_data_BMI_impute):\n",
    "    X_train = np.array(df_train['SkinThickness']).reshape(-1, 1)     \n",
    "    y_train = np.array(df_train['BMI']).reshape(-1, 1)\n",
    "    X_test = np.array(df_test['SkinThickness']).reshape(-1, 1)  \n",
    "    y_test = np.array(df_test['BMI']).reshape(-1, 1)\n",
    "# Fit linear regression\n",
    "for train_index, test_index in kf.split(pima_data_BMI_impute):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "for train_index, test_index in kf.split(pima_data_BMI_impute):\n",
    "    y_pred.append(model.predict(X_test))\n",
    "    y_true.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_square_error = np.sqrt(np.sum(np.square(np.array(y_true) - np.array(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not figure out how to get this piped into the pandas dataframe in the time I allotted for this sidequest; leaving it there for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_series = pima_data['outcome'] == 't'\n",
    "pima_data['outcome'] = outcome_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_data[['bmi', 'skinthickness', 'diabp', 'glucose', 'insulin']] = pima_data[['bmi', 'skinthickness', 'diabp', 'glucose', 'insulin']].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up multiple feature sets to look into which models are most predictive with which features\n",
    "After discussion with Richard, including people with diagnosable prediabetes (blood glucose >= 140 mg/dL) as diabetic, since those people would also need interventions.\n",
    "\n",
    "The basic \"diagnostic\" model (i.e. what might get pitched to insurance companies) can use no more than BMI, age, blood pressure, and number of pregnancies - these are data either present or collected at every well visit. For patients with a marginal diabetes risk score, diabetes pedigree function (which I assume is some sort of weighted average of close relatives with diabetes) can be obtained with a short interview ; similarly, skin thickness can be measured easily if it is found to be more informative than BMI/more informative than BMI alone.\n",
    "\n",
    "Glucose and insulin data is being treated as a positive control in this case. Glucose is directly used to diagnose patients as diabetic or prediabetic (see cell immediately below), and type II diabetes is typically a disease of insulin-resistance, where patients develop an elevated insulin level. If these don't turn out to be highly informative features in the training sets that include them, something is deeply wrong with my assumptions and/or code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diabp</th>\n",
       "      <th>skinthickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>dpf</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnancies  glucose  diabp  skinthickness  insulin   bmi    dpf  age  \\\n",
       "0              6    148.0   72.0           35.0      NaN  33.6  0.627   50   \n",
       "1              1     85.0   66.0           29.0      NaN  26.6  0.351   31   \n",
       "2              8    183.0   64.0            NaN      NaN  23.3  0.672   32   \n",
       "3              1     89.0   66.0           23.0     94.0  28.1  0.167   21   \n",
       "4              0    137.0   40.0           35.0    168.0  43.1  2.288   33   \n",
       "..           ...      ...    ...            ...      ...   ...    ...  ...   \n",
       "763           10    101.0   76.0           48.0    180.0  32.9  0.171   63   \n",
       "764            2    122.0   70.0           27.0      NaN  36.8  0.340   27   \n",
       "765            5    121.0   72.0           23.0    112.0  26.2  0.245   30   \n",
       "766            1    126.0   60.0            NaN      NaN  30.1  0.349   47   \n",
       "767            1     93.0   70.0           31.0      NaN  30.4  0.315   23   \n",
       "\n",
       "     outcome  diabetes  \n",
       "0       True      True  \n",
       "1      False     False  \n",
       "2       True      True  \n",
       "3      False     False  \n",
       "4       True      True  \n",
       "..       ...       ...  \n",
       "763    False     False  \n",
       "764    False     False  \n",
       "765    False     False  \n",
       "766     True      True  \n",
       "767    False     False  \n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima_data['diabetes'] = (pima_data['glucose'].gt(139, fill_value = False) | pima_data['outcome'].eq(True, fill_value = False))\n",
    "pima_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making feature sets to get information on which gives the best performance\n",
    "featureset1 = ['diabp', 'bmi', 'age', 'diabetes']\n",
    "featureset2 = featureset1 + ['pregnancies']\n",
    "featureset3 = featureset1 + ['dpf']\n",
    "featureset4 = featureset1 + ['glucose', 'insulin']\n",
    "featureset5 = featureset1 + ['pregnancies', 'dpf']\n",
    "featureset6 = featureset4 + ['pregnancies', 'dpf']\n",
    "featureset7 = featureset2 + ['skinthickness']\n",
    "featureset8 = featureset5 + ['skinthickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_list = [featureset1, featureset2, featureset3, featureset4, featureset5, featureset6, featureset7, featureset8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabp</th>\n",
       "      <th>bmi</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>76.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>70.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>72.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>60.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>70.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diabp   bmi  age  diabetes\n",
       "0     72.0  33.6   50      True\n",
       "1     66.0  26.6   31     False\n",
       "2     64.0  23.3   32      True\n",
       "3     66.0  28.1   21     False\n",
       "4     40.0  43.1   33      True\n",
       "..     ...   ...  ...       ...\n",
       "763   76.0  32.9   63     False\n",
       "764   70.0  36.8   27     False\n",
       "765   72.0  26.2   30     False\n",
       "766   60.0  30.1   47      True\n",
       "767   70.0  30.4   23     False\n",
       "\n",
       "[729 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima_data1 = pima_data[featureset1].dropna()\n",
    "pima_data2 = pima_data[featureset2].dropna()\n",
    "pima_data3 = pima_data[featureset3].dropna()\n",
    "pima_data4 = pima_data[featureset4].dropna()\n",
    "pima_data5 = pima_data[featureset5].dropna()\n",
    "pima_data6 = pima_data[featureset6].dropna()\n",
    "pima_data7 = pima_data[featureset7].dropna()\n",
    "pima_data8 = pima_data[featureset8].dropna()\n",
    "pima_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got my 8 datasets with varying features, differing in size because of dropped rows. For each of these, I need to split them into train and test; I need to hold out some uncontaminated data for final results following model tuning and cross-validation. I can already tell I'll want a pipeline set up so very badly by the end of this process. Brace yourselves, mateys, thar blows the biggest school of cells you salty dogs have ever seen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset1X = ['diabp', 'bmi', 'age']\n",
    "featureset2X = featureset1X + ['pregnancies']\n",
    "featureset3X = featureset1X + ['dpf']\n",
    "featureset4X = featureset1X + ['glucose', 'insulin']\n",
    "featureset5X = featureset1X + ['pregnancies', 'dpf']\n",
    "featureset6X = featureset4X + ['pregnancies', 'dpf']\n",
    "featureset7X = featureset2X + ['skinthickness']\n",
    "featureset8X = featureset5X + ['skinthickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pd1, X_test_pd1, y_train_pd1, y_test_pd1 = train_test_split(pima_data1[featureset1X], \n",
    "                                                                    pima_data1['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 17)\n",
    "\n",
    "X_train_pd2, X_test_pd2, y_train_pd2, y_test_pd2 = train_test_split(pima_data2[featureset2X], \n",
    "                                                                    pima_data2['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 8)\n",
    "X_train_pd3, X_test_pd3, y_train_pd3, y_test_pd3 = train_test_split(pima_data3[featureset3X], \n",
    "                                                                    pima_data3['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 20)\n",
    "X_train_pd4, X_test_pd4, y_train_pd4, y_test_pd4 = train_test_split(pima_data4[featureset4X], \n",
    "                                                                    pima_data4['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 36)\n",
    "X_train_pd5, X_test_pd5, y_train_pd5, y_test_pd5 = train_test_split(pima_data5[featureset5X], \n",
    "                                                                    pima_data5['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 79)\n",
    "X_train_pd6, X_test_pd6, y_train_pd6, y_test_pd6 = train_test_split(pima_data6[featureset6X], \n",
    "                                                                    pima_data6['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 11)\n",
    "X_train_pd7, X_test_pd7, y_train_pd7, y_test_pd7 = train_test_split(pima_data7[featureset7X], \n",
    "                                                                    pima_data7['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 57)\n",
    "X_train_pd8, X_test_pd8, y_train_pd8, y_test_pd8 = train_test_split(pima_data8[featureset8X], \n",
    "                                                                    pima_data8['diabetes'], \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    random_state = 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabp</th>\n",
       "      <th>bmi</th>\n",
       "      <th>age</th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>dpf</th>\n",
       "      <th>skinthickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>55.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>0.136</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>64.0</td>\n",
       "      <td>34.2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.260</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>66.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.289</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>58.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.219</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>62.0</td>\n",
       "      <td>41.2</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>0.441</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>88.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>0.262</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>86.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.306</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>74.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>0.204</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>74.0</td>\n",
       "      <td>37.4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>68.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0.299</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diabp   bmi  age  pregnancies    dpf  skinthickness\n",
       "194   55.0  24.4   42            8  0.136           20.0\n",
       "121   64.0  34.2   24            6  0.260           39.0\n",
       "721   66.0  38.1   21            1  0.289           36.0\n",
       "742   58.0  28.5   22            1  0.219           18.0\n",
       "712   62.0  41.2   38           10  0.441           36.0\n",
       "..     ...   ...  ...          ...    ...            ...\n",
       "282   88.0  32.4   37            7  0.262           15.0\n",
       "368   86.0  27.5   22            3  0.306           16.0\n",
       "161   74.0  37.2   45            7  0.204           40.0\n",
       "150   74.0  37.4   24            1  0.399           50.0\n",
       "260   68.0  30.9   34            3  0.299           15.0\n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pd8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression CV\n",
    "Fitting logistic regressors to various feature sets with 5-fold CV. Coefficients being entered into an Excel spreadsheet so I can tab over to another program that has them always at the ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1920: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.684981684981685"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV(cv = 5, scoring = 'recall_weighted', class_weight = 'balanced')\n",
    "lrcv.fit(X_train_pd1, y_train_pd1)\n",
    "lrcv.score(X_train_pd1, y_train_pd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabp</th>\n",
       "      <th>bmi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>88.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>70.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>65.0</td>\n",
       "      <td>42.6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>74.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>60.0</td>\n",
       "      <td>29.8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     diabp   bmi  age\n",
       "600   88.0  27.1   24\n",
       "493   70.0  28.9   45\n",
       "213   65.0  42.6   24\n",
       "358   74.0  35.3   48\n",
       "530   60.0  29.8   22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd2, y_train_pd2)\n",
    "lrcv.score(X_train_pd2, y_train_pd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd3, y_train_pd3)\n",
    "lrcv.score(X_train_pd3, y_train_pd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd4, y_train_pd4)\n",
    "lrcv.score(X_train_pd4, y_train_pd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd5, y_train_pd5)\n",
    "lrcv.score(X_train_pd5, y_train_pd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd6, y_train_pd6)\n",
    "lrcv.score(X_train_pd6, y_train_pd6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd7, y_train_pd7)\n",
    "lrcv.score(X_train_pd7, y_train_pd7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.fit(X_train_pd8, y_train_pd8)\n",
    "lrcv.score(X_train_pd8, y_train_pd8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('linreg1.pkl', 'wb') as fp:\n",
    "    pickle.dump(lrcv, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further work with logistic regression\n",
    "Glucose, as expected, was extremely important for logistic regression in any feature set that included it. Surprisingly, insulin wasn't similarly useful; it may be that elevated insulin levels show up later in disease/later in uncontrolled disease than elevated glucose does.\n",
    "\n",
    "Prior to presenting work, it will be necessary to scale coefficients relative to training data. I initially thought that diabetes pedigree function was extremely important, but comparison of model scores for models with and without it indicated it might not be providing much benefit; it spans a much smaller range than other measurements, hence the much larger coefficient.\n",
    "\n",
    "In this set of models, glucose+insulin provided the greatest score increase (0.18, avg of 2 comparisons), followed by skin thickness (0.03, avg of  2 comparisons), diabetes pedigree function (0.02, avg of 3 comparisons), and number of pregnancies (-0.01, avg of 3 comparisons). At this point, I doubt that any of the score increases are statistically significant; certainly none besides the glucose+insulin score increase are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors: are there complex decision boundaries?\n",
    "As we know, logistic regression isn't good with data that are separable in a non-linear fashion. KNN, however, can be excellent at that, given scaled data which is separable within the dimensions of the feature space.\n",
    "\n",
    "I expect that the problem is not a case of complex decision boundaries but rather one of data overlap; many people who do not currently have type II diabetes will go on to develop type II diabetes at a later point, and others who look like they are \"at risk\" will stave it off through diet, exercise, and other actions not captured by the features gathered. Because of this, I expect KNN to perform relatively poorly on these data. However, if KNN does perform better than logistic regression, I will need to do some more examination of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Create stratified 5-fold partitioning of training set for KNN CV\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "scaler = StandardScaler()\n",
    "kneighbors = KNeighborsClassifier(weights = 'distance')\n",
    "acc_bal_scores = []\n",
    "wt_recall_scores = []\n",
    "for train, val in skf.split(X_train_pd1, y_train_pd1):\n",
    "    # Fit scaler on training partitions\n",
    "    scaler.fit(X_train_pd1.iloc[train])\n",
    "    # Scale training and validation partitions\n",
    "    scaler.transform(X_train_pd1.iloc[train])\n",
    "    scaler.transform(X_train_pd1.iloc[val])\n",
    "    # Fit KNN\n",
    "    kneighbors.fit(X_train_pd1.iloc[train], y_train_pd1.iloc[train])\n",
    "    # Get KNN score\n",
    "    kpred = kneighbors.predict(X_train_pd1.iloc[val])\n",
    "    acc_bal_scores.append(balanced_accuracy_score(y_train_pd1.iloc[val], kpred))\n",
    "    wt_recall_scores.append(recall_score(y_train_pd1.iloc[val], kpred, average = 'weighted'))\n",
    "with open('knn1.pkl', 'wb') as fp:\n",
    "    pickle.dump(kneighbors, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_recall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy hannah that's going to be an unbearable amount of manual updating. Let's pack this inside another loop, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_to_test = [(X_train_pd2, y_train_pd2), (X_train_pd3, y_train_pd3), (X_train_pd4, y_train_pd4), (X_train_pd5, y_train_pd5), \n",
    "               (X_train_pd6, y_train_pd6), (X_train_pd7, y_train_pd7), (X_train_pd8, y_train_pd8)]\n",
    "acc_bal_dict = {}\n",
    "wt_recall_dict = {}\n",
    "count = 2\n",
    "for i in sets_to_test:\n",
    "    acc_bal_scores = []\n",
    "    wt_recall_scores = []\n",
    "    for train, val in skf.split(i[0], i[1]):\n",
    "        # Fit scaler on training partitions\n",
    "        scaler.fit(i[0].iloc[train])\n",
    "        # Scale training and validation partitions\n",
    "        scaler.transform(i[0].iloc[train])\n",
    "        scaler.transform(i[0].iloc[val])\n",
    "        # Fit KNN\n",
    "        kneighbors.fit(i[0].iloc[train], i[1].iloc[train])\n",
    "        # Get KNN score\n",
    "        kpred = kneighbors.predict(i[0].iloc[val])\n",
    "        acc_bal_scores.append(balanced_accuracy_score(i[1].iloc[val], kpred))\n",
    "        wt_recall_scores.append(recall_score(i[1].iloc[val], kpred, average = 'weighted'))\n",
    "    acc_bal_dict[count] = acc_bal_scores\n",
    "    wt_recall_dict[count] = wt_recall_scores\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_bal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it that the boundaries are really complex? Lowering number of neighbors to see.\n",
    "kneighbors = KNeighborsClassifier(n_neighbors = 2, weights = 'distance')\n",
    "acc_bal_scores = []\n",
    "wt_recall_scores = []\n",
    "for train, val in skf.split(X_train_pd1, y_train_pd1):\n",
    "    # Fit scaler on training partitions\n",
    "    scaler.fit(X_train_pd1.iloc[train])\n",
    "    # Scale training and validation partitions\n",
    "    scaler.transform(X_train_pd1.iloc[train])\n",
    "    scaler.transform(X_train_pd1.iloc[val])\n",
    "    # Fit KNN\n",
    "    kneighbors.fit(X_train_pd1.iloc[train], y_train_pd1.iloc[train])\n",
    "    # Get KNN score\n",
    "    kpred = kneighbors.predict(X_train_pd1.iloc[val])\n",
    "    acc_bal_scores.append(balanced_accuracy_score(y_train_pd1.iloc[val], kpred))\n",
    "    wt_recall_scores.append(recall_score(y_train_pd1.iloc[val], kpred, average = 'weighted'))\n",
    "acc_bal_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN results:\n",
    "As expected, KNN is doing worse than logistic regression (by 5-10% in the tests above). Lowering k led to worse performance, not better. This supports my intuition that the problem is the gradual shading from not diabetic to prediabetic to diabetic, not some kind of complex boundary shapes.\n",
    "\n",
    "Conclusion: KNN is most certainly not the right model type for this problem. Slightly duh.\n",
    "\n",
    "# Sidequest: VIF!\n",
    "Variance inflation factor is something that I keep running into in my readings and various attempts to figure out what's going on with my code. Out of curiousity and with a certain sense of dread, let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without skin thickness to preserve maximum amount of data\n",
    "vif_test = pima_data[['Pregnancies', 'Glucose', 'Insulin', 'BMI', 'BloodPressure', 'DiabetesPedigreeFunction', 'Age', 'Outcome']].dropna()\n",
    "vif_test = add_constant(vif_test)\n",
    "pd.Series([variance_inflation_factor(vif_test.values, i) \n",
    "               for i in range(vif_test.shape[1])], \n",
    "              index=vif_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With skin thickness; assumption is that skin thickness and BMI are collinear by this measure\n",
    "vif_test = pima_data[['Pregnancies', 'Glucose', 'Insulin', 'SkinThickness', 'BMI', 'BloodPressure', 'DiabetesPedigreeFunction', 'Age', 'Outcome']].dropna()\n",
    "vif_test = add_constant(vif_test)\n",
    "pd.Series([variance_inflation_factor(vif_test.values, i) \n",
    "               for i in range(vif_test.shape[1])], \n",
    "              index=vif_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code shamelessly adapted from: https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python\n",
    "Collinearity isn't a concern, yay! Also, now I know how to get this information from a dataframe in a few lines of code!\n",
    "\n",
    "# Random Forest\n",
    "May come back to SVM just in order to play with them; logreg indicates that classes aren't separable in any kind of linear way, so SVMs are unlikely to give performance that's much higher than logreg.\n",
    "\n",
    "What do we do when relationships are complicated? Random forest! (Or XGBoost, but that's coming up next.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(class_weight = 'balanced')\n",
    "sets_to_test = [(X_train_pd1, y_train_pd1), (X_train_pd2, y_train_pd2), (X_train_pd3, y_train_pd3), \n",
    "                (X_train_pd4, y_train_pd4), (X_train_pd5, y_train_pd5), (X_train_pd6, y_train_pd6), \n",
    "                (X_train_pd7, y_train_pd7), (X_train_pd8, y_train_pd8)]\n",
    "count = 1\n",
    "acc_bal_dict = {}\n",
    "wt_recall_dict = {}\n",
    "for i in sets_to_test:\n",
    "    acc_bal_scores = []\n",
    "    wt_recall_scores = []\n",
    "    for train, val in skf.split(i[0], i[1]):\n",
    "        # Fit forest\n",
    "        forest.fit(i[0].iloc[train], i[1].iloc[train])\n",
    "        # Get score\n",
    "        y_pred = forest.predict(i[0].iloc[val])\n",
    "        acc_bal_scores.append(balanced_accuracy_score(i[1].iloc[val], y_pred))\n",
    "        wt_recall_scores.append(recall_score(i[1].iloc[val], y_pred, average = 'weighted'))\n",
    "    acc_bal_dict[count] = acc_bal_scores\n",
    "    wt_recall_dict[count] = wt_recall_scores\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(class_weight = 'balanced')\n",
    "for train, val in skf.split(X_train_pd1, y_train_pd1):\n",
    "    # Fit forest\n",
    "    forest.fit(X_train_pd1.iloc[train], y_train_pd1.iloc[train])\n",
    "    # Get  score\n",
    "    y_pred = forest.predict(X_train_pd1.iloc[val])\n",
    "with open('forest1.pkl', 'wb') as fp:\n",
    "    pickle.dump(forest, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_recall_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest isn't performing as well as I would have hoped, given what I know about it. Of course, I just jumped right in without considering how to deal with the class imbalance, so shame on me.\n",
    "\n",
    "Changing class_weight on forest to = 'balanced' had little effect on model performance; some sets were better and others were worse, though the difference was certainly not statistically significant.\n",
    "\n",
    "# Boosting into orbit! AdaBoost via EasyEnsebleClassifier\n",
    "This is a very small dataset, so the relative slowness of AdaBoost isn't going to be a major consideration. Also, there are fewer parameters to tune for AdaBoost, so this will be somewhat less challenging to tune than XGBoost. Feature importance can also be output, which may help me understand what's going on under the hood. Noise is known to be an issue for AdaBoost, so I expect it to perform not much better than logistic regression (if at all better) once tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "model = AdaBoostClassifier()\n",
    "acc_bal_dict = {}\n",
    "wt_recall_dict = {}\n",
    "feat_imp_dict = {}\n",
    "for i in sets_to_test:\n",
    "    acc_bal_scores = []\n",
    "    wt_recall_scores = []\n",
    "    for train, val in skf.split(i[0], i[1]):\n",
    "        # Fit AdaBoost\n",
    "        model.fit(i[0].iloc[train], i[1].iloc[train])\n",
    "        # Get KNN score\n",
    "        y_pred = model.predict(i[0].iloc[val])\n",
    "        acc_bal_scores.append(balanced_accuracy_score(i[1].iloc[val], y_pred))\n",
    "        wt_recall_scores.append(recall_score(i[1].iloc[val], y_pred, average = 'weighted'))\n",
    "    acc_bal_dict[count] = acc_bal_scores\n",
    "    wt_recall_dict[count] = wt_recall_scores\n",
    "    feat_imp_dict[count] = model.feature_importances_\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_bal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['recall', 'balanced_accuracy']\n",
    "param_grid = {'n_estimators': range(40, 160, 20),\n",
    "             'learning_rate': uniform(0.1, 1)}\n",
    "rscv = RandomizedSearchCV(AdaBoostClassifier(), param_distributions = param_grid, n_iter = 50, scoring = scoring, \n",
    "                            cv = 5, refit = 'recall', error_score=0)\n",
    "\n",
    "best_params_dict = {}\n",
    "best_score_dict = {}\n",
    "count = 1\n",
    "for i in sets_to_test:\n",
    "    search = rscv.fit(i[0], i[1])\n",
    "    best_params_dict[count] = search.best_params_\n",
    "    best_score_dict[count] = search.best_score_\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, AdaBoost wasn't as good as random forest. Definitely SLOW compared to other classifiers (though not terrible for this dataset - took a few minutes to do the randomized grid search for parameters).\n",
    "\n",
    "# XGBoost and why I'm throwing all of this at the wall\n",
    "Finally we get to the one that I was pretty sure I was going to end up with all along: XGBoost. But performance on the Pima Indianse Diabetes dataset isn't going to be the real test of this model, even if I can get a much better recall. What's going to be the real test is training and then pickling the models on a relatively minimal feature set (i.e. diastolic, BMI, age) and then testing them against the train subset of the Framingham data that I have. Whichever model is the most predictive with that data can be the one to finally get trained on the mixed training dataset and tested against the mixed test dataset for final scoring. If there isn't a clear winner, I can choose one that's fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting baseline xgb performance\n",
    "xgbc = xgb.XGBClassifier()\n",
    "acc_bal_scores = []\n",
    "wt_recall_scores = []\n",
    "for train, val in skf.split(X_train_pd1, y_train_pd1):\n",
    "    xgbc.fit(X_train_pd1.iloc[train], y_train_pd1.iloc[train])\n",
    "    y_pred = xgbc.predict(X_train_pd1.iloc[val])\n",
    "    acc_bal_scores.append(balanced_accuracy_score(y_train_pd1.iloc[val], y_pred))\n",
    "    wt_recall_scores.append(recall_score(y_train_pd1.iloc[val], y_pred, average = 'weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5968070652173914,\n",
       " 0.6355298913043479,\n",
       " 0.5811820652173914,\n",
       " 0.6222222222222222,\n",
       " 0.6603174603174603]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_bal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_recall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is a pretty poor showing so far. First thing to try: correcting for imbalance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:43] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:32:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1593723618214/work/src/learner.cc:480: \n",
      "Parameters: { scale_pos_weights } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'scale_pos_weights': [1, 2, 4, 8, 16]}\n",
    "gscv = GridSearchCV(estimator = xgbc, param_grid = param_grid, cv = skf, scoring = 'recall')\n",
    "gscv_result = gscv.fit(X_train_pd1, y_train_pd1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scale_pos_weights': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if the best class weight is 1, that would seem to indicate that tuning the class weight hyperparameter is a non-starter. What else needs tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3], \n",
    "             'eval_metric': ['error', 'auc']}\n",
    "gscv = GridSearchCV(estimator = xgbc, param_grid = param_grid, cv = skf, scoring = 'recall')\n",
    "gscv_result = gscv.fit(X_train_pd1, y_train_pd1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chesh1/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5788979136805225"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05], \n",
    "             'eval_metric': ['error', 'auc']}\n",
    "gscv = GridSearchCV(estimator = xgbc, param_grid = param_grid, cv = skf, scoring = 'recall')\n",
    "gscv_result = gscv.fit(X_train_pd1, y_train_pd1)\n",
    "gscv_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_metric': 'error', 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.01],\n",
    "              'n_estimators': [20, 40, 60, 80, 100],\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "gscv = GridSearchCV(estimator = xgbc, param_grid = param_grid, cv = skf, scoring = 'recall')\n",
    "gscv_result = gscv.fit(X_train_pd1, y_train_pd1)\n",
    "gscv_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
